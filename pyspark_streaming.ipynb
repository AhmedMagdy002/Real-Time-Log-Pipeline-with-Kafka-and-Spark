{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the exact package versions for Spark 3.0.1\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.kafka:kafka-clients:2.4.1 pyspark-shell'\n",
    "\n",
    "# Then create your SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_extract, window, count, current_timestamp\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Initialize Spark Session \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ApacheLogAnalysis60SecIntervals\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce notebook output noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log_line: string (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      " |-- processing_time: timestamp (nullable = false)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- bytes: integer (nullable = true)\n",
      "\n",
      "Query 'total_requests' is active.\n",
      "Query 'status_distribution' is active.\n",
      "Query 'method_distribution' is active.\n",
      "Query 'ip_traffic' is active.\n",
      "Query 'error_rates' is active.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"apache_logs\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Extract the log message from Kafka value\n",
    "logs_df = kafka_df.selectExpr(\n",
    "    \"CAST(value AS STRING) as log_line\",\n",
    "    \"CAST(timestamp AS TIMESTAMP) as kafka_timestamp\"\n",
    ")\n",
    "\n",
    "# Add processing timestamp for windowing\n",
    "logs_with_time = logs_df.withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "# Parse Apache log fields using regex patterns\n",
    "parsed_logs = logs_with_time \\\n",
    "    .withColumn(\"ip\", regexp_extract(\"log_line\", r\"^(\\S+)\", 1)) \\\n",
    "    .withColumn(\"timestamp\", regexp_extract(\"log_line\", r\"$$(\\S+ [+\\-]\\d{4})$$\", 1)) \\\n",
    "    .withColumn(\"method\", regexp_extract(\"log_line\", r\"\\\"(\\S+) \", 1)) \\\n",
    "    .withColumn(\"endpoint\", regexp_extract(\"log_line\", r\"\\\"(?:\\S+) (\\S+) \", 1)) \\\n",
    "    .withColumn(\"protocol\", regexp_extract(\"log_line\", r\"\\\"(?:\\S+) (?:\\S+) (\\S+)\\\"\", 1)) \\\n",
    "    .withColumn(\"status\", regexp_extract(\"log_line\", r\" (\\d{3}) \", 1).cast(IntegerType())) \\\n",
    "    .withColumn(\"bytes\", regexp_extract(\"log_line\", r\" (\\d+|-) \", 1).cast(IntegerType()))\n",
    "\n",
    "# Display schema in notebook\n",
    "parsed_logs.printSchema()\n",
    "\n",
    "# Group data into 60-second windows based on processing time\n",
    "windowed_logs = parsed_logs \\\n",
    "    .groupBy(window(col(\"processing_time\"), \"60 seconds\")) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"total_requests\")\n",
    "\n",
    "# Status code distribution in 60-second windows\n",
    "status_distribution = parsed_logs \\\n",
    "    .groupBy(window(col(\"processing_time\"), \"60 seconds\"), \"status\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\", \"status\")\n",
    "\n",
    "# Endpoint traffic in 60-second windows\n",
    "endpoint_traffic = parsed_logs \\\n",
    "    .groupBy(window(col(\"processing_time\"), \"60 seconds\"), \"endpoint\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\", col(\"count\").desc())\n",
    "\n",
    "# HTTP method distribution in 60-second windows\n",
    "method_distribution = parsed_logs \\\n",
    "    .groupBy(window(col(\"processing_time\"), \"60 seconds\"), \"method\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\", col(\"count\").desc())\n",
    "\n",
    "# IP traffic in 60-second windows (top IPs)\n",
    "ip_traffic = parsed_logs \\\n",
    "    .groupBy(window(col(\"processing_time\"), \"60 seconds\"), \"ip\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\", col(\"count\").desc())\n",
    "\n",
    "# Error rates in 60-second windows\n",
    "error_rates = parsed_logs \\\n",
    "    .filter(col(\"status\") >= 400) \\\n",
    "    .groupBy(window(col(\"processing_time\"), \"60 seconds\"), \"status\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"window\", \"status\")\n",
    "\n",
    "# Define a function to start streaming queries in notebook\n",
    "def start_query(df, name, num_rows=20):\n",
    "    return df.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .trigger(processingTime=\"60 seconds\") \\\n",
    "        .queryName(name) \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .option(\"numRows\", num_rows) \\\n",
    "        .start()\n",
    "\n",
    "# Start all queries\n",
    "queries = []\n",
    "\n",
    "# Total requests per 60 seconds\n",
    "queries.append(start_query(windowed_logs, \"total_requests\"))\n",
    "\n",
    "# Status distribution\n",
    "queries.append(start_query(status_distribution, \"status_distribution\"))\n",
    "\n",
    "# Endpoint traffic\n",
    "queries.append(start_query(endpoint_traffic, \"endpoint_traffic\"))\n",
    "\n",
    "# Method distribution\n",
    "queries.append(start_query(method_distribution, \"method_distribution\"))\n",
    "\n",
    "# IP traffic\n",
    "queries.append(start_query(ip_traffic, \"ip_traffic\"))\n",
    "\n",
    "# Error rates\n",
    "queries.append(start_query(error_rates, \"error_rates\"))\n",
    "\n",
    "# Display active queries\n",
    "for query in queries:\n",
    "    print(f\"Query '{query.name}' is active.\")\n",
    "\n",
    "# To stop all queries (run in a separate cell when ready)\n",
    "# for query in queries:\n",
    "#     query.stop()\n",
    "# print(\"All streaming queries stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All streaming queries stopped.\n"
     ]
    }
   ],
   "source": [
    "#To stop all queries (run in a separate cell when ready)\n",
    "for query in queries:\n",
    "    \n",
    "    query.stop()\n",
    "print(\"All streaming queries stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
